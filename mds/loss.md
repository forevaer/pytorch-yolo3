# 预测内容
神经网络肯定是有目的的进行学习，`YOLO`的学习目标就有如下几点
- 坐标
- 宽高
- 分类
- 前景

后面我们就分别来叙述一下。
# 前景
目标检测的最初感觉应该是`目标检测+类别回归`，但实际上，或者说在`YOLO`中，都会存在一个新的分类，那就是非分类。<br>
或者说是非分类目标`others`。或者还有一种方法
- 前景
- 背景

也就是说，在具体进行详细的类别划分之前，需要经过一个而分类，然后再进行后续的详细分类判断。<br>
前景、背景，两者其实是同一个分类条件，非此即彼，但同时计算损失，对于分类精度能大大提高。

# 分类
分类就不必说了，详细的情况的话参考一下分类网络即可。


# 比例
可能，在大部分的说明当中，都会缺失这一点，也就是`anchor`的计算。<br>
对于预测的边框的放大，大家都无师自通，但是，始源的操作，却得反复提及这个中心思想
> `anchor`是在`FeatureMap`上进行对比的。

# 坐标

坐标重要么，真的不重要。<br>
如果说按照缩放比例的话，坐标本身就是确定的(嗯，整数上)，所以，关键在于准确的偏移。<br>
也就是小数点后面的数字，嗯？零到一之间，所以可以用`sigmod`来进行回归。

# 宽高

问题来了，坐标通过映射都无误，只会有微小的误差，何况宽高呢。<br>
也就是说，通过控制宽高的比例，就能够更精确的去描绘`FeatureMap`上的`anchor`了。

# 框图回归

总结一下框图的目标变换
1. 拟合框图: 直接得到结果
2. 拟合`FeatureMap`上的框图: 比例固定，`FeatureMap`上预测即可
3. 拟合`FeatureMap`上的框图偏移: 比例固定，整数固定，预测偏移即可


# 总损失

```python
 total_loss = loss_x + loss_y + loss_w + loss_h + loss_class + loss_object
```
这里只做总体的概述，详细细节，代码后续说明。

> 损失代码参看： [YOLONet.py](../net/YOLONet.py) <br>
> 详细属性参看： [utils.py](../assist/compute.py)